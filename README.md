# phy format

**This is an early draft.**


* Root directory: `/home/user/spikesorting/20160101/`
* All paths below are relative to this directory.

## Raw data

### Files

```
20160101_1.ns5
20160101_2.dat
```

### API

Looking at the traces in the terminal:

```
phy traces 20160101_1.ns5
phy traces 20160101_2.dat --n-channels=128 --dtype=int16 --sample-rate=20000
```

Looking at the traces in IPython:

```python
import phy.io
import phy.plot as plt

traces = phy.io.read_traces(['20160101_1.ns5', '20160101_2.dat'])
plt.plot_traces(traces)
plt.show()
```


## Experiment files

* The subdirectory `20160101.phy/`, named **phy directory**, represents a single dataset.
* `20160101.phy/20160101.metadata` is the **main metadata file**.
* All `.metadata` files in subdirectories are recursively merged in a single JSON object.
    * TODO: how to deal with conflicts? I suggest to raise an error (strict behavior).
    * TODO: how to deal with collections containing multiple objects with the same key?
* Every object has a unique id which **must be automatically generated by pymongo**.

Some metadata objects (by default, all are in the main metadata file):

```
{"experiment": {
    "_id": 098274365020,
    "subject_name": "ns20141202",
    "datetime": "2016-01-01T11:01:37Z"  # mongodb uses ISO 8601
    }
}

{"traces": {
    "experiment_id": 098274365020,
    "files": ["traces/20160101_1.ns5",
              "traces/20160101_2.dat"]
    }
}

{"spikes": {
    "_id": 102784512155,  # there can be multiple spikes objects = multiple runs of SpikeDetekt
    "experiment_id": 098274365020
    "spike_times": {"path: "spike_times/"}, # an array or an object with "path"
    "spike_widths: [0.34, 0.57, 0.34, ...],
    }
}

{"clusters": {
    "_id": 987513216832,  # there can be multiple clusters objects = multiple clusterings
    "experiment_id": 098274365020
    "spike_clusters": {"path": "spike_clusters/"},
    "cluster_groups": [0, 2, 1, 0, 3, 1, 0, ...]
    }
}

# TODO: a JSON object **cannot** contain two objects with the same key
* but mongoDB can: it's the concept of collection
* in which files should we store these objects?
{"clusters": {
    "_id": 129898408501,  # there can be multiple clusters objects = multiple clusterings
    "experiment_id": 098274365020
    "spike_clusters": {"path": "spike_clusters_klustakwik/"},
    }
}
```


## Data arrays

* An array is either:
    * a JSON array,
    * or an object with a `path` attribute.

* The `path` refers to a file
* The `path` is relative to the phy directory.
* Every array is defined by one file
* The file extension defines the format for the array
* Supported formats: txt, npy, ns5, Neo formats <http://neo.readthedocs.org/en/latest/io.html>...
* New tar format: first archived file is format.ini with comment with link to description, array description, then one or several flat binary files
    * array description in ini: dtype, shape, labels

## API

* TODO: how the user specifies the parameters/probe (PRM? PRB)? Where these files should be stored? Should the parameters be imported in the JSON? => duplication problem. Any traces/spike detection/clustering parameter should be stored in a single file.
